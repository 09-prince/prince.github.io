<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>RLHF vs DPO - Prince Gour</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link rel="stylesheet" href="../style.css">
  <style>
    article {
      max-width: 900px;
      margin: 0 auto;
      padding: 40px 20px;
    }

    article h1 {
      font-size: 2em;
      margin-bottom: 15px;
      line-height: 1.3;
    }

    .article-meta {
      color: var(--text-secondary);
      font-size: 0.9em;
      margin-bottom: 40px;
      padding-bottom: 20px;
      border-bottom: 1px solid var(--border-color);
    }

    article h2 {
      font-size: 1.5em;
      margin-top: 50px;
      margin-bottom: 20px;
    }

    article h3 {
      font-size: 1.2em;
      margin-top: 30px;
      margin-bottom: 15px;
      color: var(--text-color);
    }

    article p {
      margin-bottom: 20px;
      line-height: 1.8;
    }

    article ul {
      margin-bottom: 20px;
      padding-left: 30px;
    }

    article ul li {
      margin-bottom: 10px;
      line-height: 1.8;
    }

    article a {
      color: var(--accent-color);
      text-decoration: none;
    }

    article a:hover {
      text-decoration: underline;
    }

    pre {
      background: var(--hover-bg);
      color: var(--text-color);
      padding: 20px;
      border-radius: 8px;
      overflow-x: auto;
      font-size: 0.9em;
      line-height: 1.6;
      margin: 25px 0;
      border: 1px solid var(--border-color);
    }

    code {
      background: var(--hover-bg);
      padding: 2px 6px;
      border-radius: 4px;
      font-size: 0.95em;
      font-family: 'Courier New', monospace;
    }

    .article-links {
      margin-top: 40px;
      padding: 25px;
      background: var(--hover-bg);
      border-radius: 8px;
      border: 1px solid var(--border-color);
    }

    .article-links h3 {
      margin-top: 0;
    }

    .article-links a {
      display: block;
      margin-bottom: 10px;
    }
  </style>
</head>
<body>
  <!-- Navigation -->
  <nav>
    <div class="nav-container">
      <div class="nav-social">
        <a href="mailto:gourprince2004@gmail.com" title="Email"><i class="fa-solid fa-envelope"></i></a>
        <a href="https://github.com/09-prince" target="_blank" title="GitHub"><i class="fa-brands fa-github"></i></a>
        <a href="https://linkedin.com/in/yourprofile" target="_blank" title="LinkedIn"><i class="fa-brands fa-linkedin"></i></a>
        <a href="https://twitter.com/yourusername" target="_blank" title="X"><i class="fa-brands fa-x-twitter"></i></a>
      </div>

      <button class="mobile-toggle" onclick="toggleMobileMenu()"><i class="fa-solid fa-bars"></i></button>

      <ul class="nav-links" id="navLinks">
        <li><a href="../index.html">about</a></li>
        <li><a href="index.html" class="active">blog</a></li>
        <li><a href="../cv/index.html">cv</a></li>
        <li>
          <button class="theme-toggle" onclick="toggleTheme()" title="Toggle theme">
            <i class="fa-solid fa-moon"></i>
            <i class="fa-solid fa-sun"></i>
          </button>
        </li>
      </ul>
    </div>
  </nav>

  <!-- Article Content -->
  <article>
    <a href="index.html" class="back-link">
      <i class="fa-solid fa-arrow-left"></i> Back to Blog
    </a>

    <h1>Reinforcement Learning from Human Feedback (RLHF) and the Rise of DPO</h1>
    <div class="article-meta">By Prince Gour · October 2025</div>

    <h2>RLHF: The Traditional Approach</h2>
    <p>Modern large language models (LLMs) are first trained on vast amounts of text, but steering them to behave how we want requires extra tuning. The most common method is <strong>Reinforcement Learning from Human Feedback (RLHF)</strong>.</p>

    <p>In RLHF, the model's outputs are judged by humans, a separate reward model is trained to predict those human preferences, and then the LLM is fine-tuned with reinforcement learning to maximize that learned reward. For example:</p>

    <ul>
      <li>Pre-train a base model on general text (next-word prediction)</li>
      <li>Generate answers: For each prompt, the model creates several possible responses</li>
      <li>Human feedback: People rank those responses or pick which one they prefer</li>
      <li>Train a reward model: A new network learns to give high scores to the preferred answers</li>
      <li>Reinforce and fine-tune: Using an RL algorithm (often PPO), the model is updated to produce answers that get higher reward scores while staying close to its original behavior</li>
    </ul>

    <p>While this approach can yield helpful, conversation-ready models, it is complex and costly. It requires training two models, sampling many outputs, and balancing reward optimization versus safety. Tuning these systems can be unstable, and the separate reward model can cause "reward hacking," where the model finds loopholes in the reward signal.</p>

    <p><strong>In short:</strong> RLHF is powerful but complicated, compute-heavy, and finicky.</p>

    <h2>Enter DPO: A Simpler Approach</h2>
    <p><strong>Direct Preference Optimization (DPO)</strong> achieves the same goal – aligning models with human preferences – but without reinforcement learning. DPO skips the separate reward model entirely. Instead, it directly adjusts the language model so that it assigns higher probability to preferred responses and lower to less-preferred ones.</p>

    <p>DPO transforms RLHF into a simpler supervised learning problem. Instead of learning a reward, the model learns from comparisons directly using a <em>binary cross-entropy loss</em> – pushing up probabilities of preferred answers and down for rejected ones. Despite this simplicity, DPO is mathematically equivalent to RLHF's optimization, without the RL machinery.</p>

    <h2>How DPO Works (An Intuitive Example)</h2>
    <p>Imagine you have an AI that writes movie reviews, and you want it to write positive ones. Using DPO, you could prompt the model twice, have it generate two reviews, and pick your favorite. Over time, it learns patterns humans prefer, e.g., "positive language = better."</p>

    <p>For each prompt <em>x</em> with two completions <em>y₁</em> (good) and <em>y₂</em> (less good), DPO increases <em>P(y₁|x)</em> and decreases <em>P(y₂|x)</em>, all while keeping the model stable. This avoids sampling loops or RL randomness.</p>

    <h2>Advantages of DPO</h2>
    <ul>
      <li><strong>Fewer moving parts:</strong> Only one model to fine-tune; no reward network or PPO loop</li>
      <li><strong>Stability:</strong> No RL training instabilities or reward hacking</li>
      <li><strong>Lower cost:</strong> No repeated sampling or double networks – much faster</li>
      <li><strong>Performance:</strong> Matches or beats RLHF in alignment and dialogue quality</li>
    </ul>

    <p>Large-scale results confirm this: open models trained with DPO have surpassed GPT-3.5 on alignment benchmarks. DPO scales cleanly to 70B parameters and remains stable – proving it's not just a small-scale trick but a practical alignment tool.</p>

    <h2>DPO in Action: Experiment Results</h2>
    <p>The DPO paper showed results on tasks like sentiment control, summarization, and dialogue. For example:</p>
    <ul>
      <li>IMDb sentiment task: DPO generated more positive reviews than RLHF models</li>
      <li>Reddit TL;DR summarization: DPO preferred 61% of the time vs. PPO's 57%</li>
      <li>Dialogue: matched or exceeded top RLHF baselines</li>
    </ul>
    <p>In every case, DPO trained faster and needed less hyperparameter tuning, showing a simpler method can yield better or equal results.</p>

    <h2>Why DPO Matters</h2>
    <p>DPO represents a major step in AI alignment. By collapsing RLHF's multi-stage pipeline into one simple learning task, it allows developers to train human-aligned models more easily and cheaply.</p>
    <p>DPO reduces engineering overhead while maintaining high alignment quality. As one review summarized: <em>"DPO offers a simple and efficient alternative for aligning language models with human preferences."</em></p>

    <p>In practice, DPO enables faster development of safer, more reliable AI systems – from chatbots to summarizers – by directly learning from human-labeled comparisons.</p>

    <h2>Practical Implementation (Code Overview)</h2>
    <p>Below is a summarized implementation pipeline for DPO fine-tuning using Hugging Face and TRL:</p>

    <pre><code># 1. Install dependencies
!pip install trl bitsandbytes accelerate

# 2. Import packages
import torch
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import DPOTrainer, DPOConfig
from peft import LoraConfig

# 3. Load model and tokenizer
model_name = "meta-llama/Llama-2-7b-hf"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    load_in_4bit=True,
)

# 4. Load and preprocess dataset
def split_prompt_response(example):
    sep = "\n\nAssistant:"
    prompt = example["chosen"].split(sep)[0] + sep
    return {
        "prompt": prompt,
        "chosen": example["chosen"].split(sep)[1].strip(),
        "rejected": example["rejected"].split(sep)[1].strip(),
    }

dataset = load_dataset("Anthropic/hh-rlhf", split="train[:1%]")
dataset = dataset.map(split_prompt_response)

# 5. Setup configs and train
peft_config = LoraConfig(
    r=8, 
    lora_alpha=16, 
    lora_dropout=0.05, 
    task_type="CAUSAL_LM"
)

training_args = DPOConfig(
    max_steps=100,
    beta=0.1,
    output_dir="./dpo_llama",
)

trainer = DPOTrainer(
    model=model,
    ref_model=None,
    args=training_args,
    train_dataset=dataset,
    processing_class=tokenizer,
    peft_config=peft_config,
)

trainer.train()
trainer.save_model("./dpo_llama/final_checkpoint")</code></pre>

    <div class="article-links">
      <h3>Resources</h3>
      <p>
        <a href="https://arxiv.org/pdf/2305.18290" target="_blank">
          <i class="fa-solid fa-file-pdf"></i> Read the DPO Paper (arXiv)
        </a>
        <a href="https://github.com/09-prince/RLHF" target="_blank">
          <i class="fa-brands fa-github"></i> GitHub: RLHF Repository
        </a>
      </p>
    </div>
  </article>

  <footer>
    © 2025 Prince Gour · <a href="../index.html">Back to Portfolio</a>
  </footer>

  <script src="../script.js"></script>
</body>
</html>